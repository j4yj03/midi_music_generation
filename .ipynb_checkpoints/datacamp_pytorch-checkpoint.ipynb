{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d927a49-faaf-4c9e-b2b2-ce76035dfb78",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae982358-8b7c-4ac0-a900-2954b43ca32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Define the three linear layers\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass x through linear layers adding activations\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b0fc8b-f977-406f-9334-d74803f048a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "# Set up binary accuracy metric\n",
    "acc = Accuracy(task=\"binary\", num_classes=2)\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for features, labels in dataloader_test:\n",
    "        # Get predicted probabilities for test data batch\n",
    "        outputs = net(features)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        acc(preds, labels.view(-1, 1))\n",
    "\n",
    "# Compute total test accuracy\n",
    "test_accuracy = acc.compute()\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e7b44-f68e-4dfd-8bf9-346f2d0b7435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9036fcc-04cb-4f6a-b0b5-4f27e4669d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        \n",
    "        # Apply He initialization\n",
    "        init.kaiming_uniform_(self.fc1.weight)\n",
    "        init.kaiming_uniform_(self.fc2.weight)\n",
    "        init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Update ReLU activation to ELU\n",
    "        x = nn.functional.elu(self.fc1(x))\n",
    "        x = nn.functional.elu(self.fc2(x))\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e4222-4cf1-49da-95d1-4dd364e70d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 1)\n",
    "        # Add two batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.bn2 = nn.BatchNorm1d(8)\n",
    "        \n",
    "        init.kaiming_uniform_(self.fc1.weight)\n",
    "        init.kaiming_uniform_(self.fc2.weight)\n",
    "        init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\") \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.elu(x)\n",
    "\n",
    "        # Pass x through the second set of layers\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.functional.elu(x)\n",
    "\n",
    "        x = nn.functional.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ad7ea-3393-4576-ac96-f57d15df1101",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    # Add horizontal flip and rotation\n",
    "# Define transforms\n",
    "    train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomAutocontrast(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128)),\n",
    "])\n",
    "\n",
    "dataset_train = ImageFolder(\n",
    "  \"clouds_train\",\n",
    "  transform=train_transforms,\n",
    ")\n",
    "dataloader_train = DataLoader(\n",
    "  dataset_train, shuffle=True, batch_size=16\n",
    ")\n",
    "\n",
    "dataset_train = ImageFolder(\n",
    "  \"clouds_train\",\n",
    "  transform=train_transforms,\n",
    ")\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "  dataset_train, shuffle=True, batch_size=1\n",
    ")\n",
    "\n",
    "image, label = next(iter(dataloader_train))\n",
    "# Reshape the image tensor\n",
    "image = image.squeeze().permute(1, 2, 0) \n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e703d-1d3e-449b-bb63-b75c6d9ac45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Define feature extractor\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        # Define classifier\n",
    "        self.classifier = nn.Linear(64*16*16, num_classes)\n",
    "    \n",
    "    def forward(self, x):  \n",
    "        # Pass input through feature extractor and classifier\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4324321-6ad8-4b80-b184-4c8a6864b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "net = Net(num_classes=7)\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    # Iterate over training batches\n",
    "    for images, labels in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader_train)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ed3dc-580a-4c95-a823-c4e7adf1307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics\n",
    "metric_precision = Precision(task='multiclass', num_classes=7, average='micro')\n",
    "metric_recall = Recall(task='multiclass', num_classes=7, average='micro')\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader_test:\n",
    "        outputs = net(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        metric_precision(preds, labels)\n",
    "        metric_recall(preds, labels)\n",
    "\n",
    "precision = metric_precision.compute()\n",
    "recall = metric_recall.compute()\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5390d30-9b23-4fb3-bd9f-1467ff6f9042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define precision metric\n",
    "metric_precision = Precision(\n",
    "    task=\"multiclass\", num_classes=7, average=None\n",
    ")\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader_test:\n",
    "        outputs = net(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        metric_precision(preds, labels)\n",
    "precision = metric_precision.compute()\n",
    "\n",
    "# Get precision per class\n",
    "precision_per_class = {\n",
    "    k: precision[v].item()\n",
    "    for k, v \n",
    "    in dataset_test.class_to_idx.items()\n",
    "}\n",
    "print(precision_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec7545d-7390-4372-b0bf-546ad76beea9",
   "metadata": {},
   "source": [
    "### 1. Handling sequences with PyTorch\n",
    "We've learned to handle tabular and image data. Let's now discuss sequential data.\n",
    "\n",
    "### 2. Sequential data\n",
    "Sequential data is ordered in time or space, where the order of the data points is important and can contain temporal or spatial dependencies between them. Time series, data recorded over time like stock prices, weather, or daily sales is sequential. So is text, in which the order of words in a sentence determines its meaning. Another example is audio waves, where the order of data points is crucial to the sound reproduced when the audio file is played.\n",
    "\n",
    "### 3. Electricity consumption prediction\n",
    "In this chapter, we will tackle the problem of predicting electricity consumption based on past patterns. We will use a subset of the electricity consumption dataset from the UC Irvine Machine Learning Repository. It contains electricity consumption in kilowatts, or kW, for a certain user recorded every 15 minutes for four years.\n",
    "\n",
    "### 4. Train-test split\n",
    "In many machine learning applications, one randomly splits the data into training and testing sets. However, with sequential data, there are better approaches. If we split the data randomly, we risk creating a look-ahead bias, where the model has information about the future when making forecasts. In practice, we won't have information about the future when making predictions, so our test set should reflect this reality. To avoid the look-ahead bias, we should split the data by time. We will train on the first three years of data, and test on the fourth year.\n",
    "\n",
    "### 5. Creating sequences\n",
    "To feed the training data to the model, we need to chunk it first to create sequences that the model can use as training examples. First, we need to select the sequence length, which is the number of data points in one training example. Let's make each forecast based on the previous 24 hours. Because data is at 15 minute intervals, we need to use 24 times 4 which is 96 data points. In each example, the data point right after the input sequence will be the target to predict.\n",
    "\n",
    "### 6. Creating sequences in Python\n",
    "Let's implement a Python function to create sequences. It takes the DataFrame and the sequence length as inputs. We start with initializing two empty lists, xs for inputs and ys for targets. Next, we iterate over the DataFrame. The loop only goes up to \"len(df) - seq_length\", ensuring that for every iteration, there are always seq_length data points available in the DataFrame for creating the sequence and a subsequent data point to serve as the target. For each considered data point, we define inputs x as the electricity consumption values starting from this point plus the next sequence length points, and the target y as the subsequent electricity consumption value. The 1 passed to the iloc method stands for the second DataFrame column, which stores the electricity consumption data. Finally, we append the inputs and the target to pre-initialized lists, and after the loop, return them as NumPy arrays.\n",
    "\n",
    "### 7. TensorDatasetet\n",
    "Let's use our function to create sequences from the training data. This gives us almost 35 thousand training examples. To convert them to a torch Dataset, we will use the TensorDataset function. We pass it two arguments, the inputs and the targets. Each argument is the NumPy array converted to a tensor with torch.from_numpy and parsed to float. The TensorDataset behaves just like all other torch Datasets and it can be passed to a DataLoader in the same way.\n",
    "\n",
    "### 8. Applicability to other sequential datat\n",
    "Everything we have learned here can also be applied to other sequential data. For example, Large Language Models are trained to predict the next word in a sentence, a problem similar to predicting the next amount of electricity used. For speech recognition, which means transcribing an audio recording of someone speaking to text, one would typically use the same sequence-processing model architectures we will learn about soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4baa1f-50c0-4a87-ab22-18d553087eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    # Iterate over data indices\n",
    "    for i in range(len(df) - seq_length):\n",
    "      \t# Define inputs\n",
    "        x = df.iloc[i : (i+seq_length), 1]\n",
    "        # Define target\n",
    "        y = df.iloc[(i+seq_length), 1]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa0df01-9da9-43d3-a3dd-565ffa10b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Use create_sequences to create inputs and targets\n",
    "X_train, y_train = create_sequences(train_data, 24*4)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# Create TensorDataset\n",
    "dataset_train = TensorDataset(\n",
    "    torch.from_numpy(X_train).float(),\n",
    "    torch.from_numpy(y_train).float(),\n",
    ")\n",
    "print(len(dataset_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b91a7b-db6b-44c2-8328-ddeb9c8b75a5",
   "metadata": {},
   "source": [
    "### 2. Recurrent neuron\n",
    "So far, we built feed-forward neural networks where data is passed in one direction: from inputs, through all the layers, to the outputs. Recurrent neural networks, or RNNs, are similar, but also have connections pointing back. At each time step, a recurrent neuron receives some input x, multiplied by the weights and passed through an activation. Out come two values: the main output y, and the hidden state, h, that is fed back to the same neuron. In PyTorch, a recurrent neuron is available as nn.RNN.\n",
    "\n",
    "### 3. Unrolling recurrent neuron through time\n",
    "We can represent the same neuron once per time step, a visualization known as unrolling a neuron through time. At a given time step, the neuron represented as a gray circle receives input data x-zero and the previous hidden state h0 and produces output y-zero and a hidden state h1.\n",
    "\n",
    "### 4. Unrolling recurrent neuron through time\n",
    "At the next time step, it takes the next value x1 as input and its last hidden state, h1.\n",
    "\n",
    "### 5. Unrolling recurrent neuron through time\n",
    "And so it continues until the end of the input sequence. Since at the first time step there is no previous hidden state, h0 is typically set to zero. Notice that the output at each time step depends on all the previous inputs. This allows recurrent networks to maintain memory through time, which allows them to handle sequential data well.\n",
    "\n",
    "### 6. Deep RNNs\n",
    "We can also stack multiple layers of recurrent cells on top of each other to get a deep recurrent neural network. In this case, each input will pass through multiple neurons one after another, just like in dense and convolutional networks we have discussed before.\n",
    "\n",
    "### 7. Sequence-to-sequence architecture\n",
    "Depending on the lengths of input and output sequences, we distinguish four different architecture types. Let's look at them one by one. In a sequence-to-sequence architecture, we pass the sequence as input and make use of the output produced at every time step. For example, a real-time speech recognition model could receive audio at each time step and output the corresponding text.\n",
    "\n",
    "### 8. Sequence-to-vector architecture\n",
    "In a sequence-to-vector architecture, we pass a sequence as input, but ignore all the outputs but the last one. In other words, we let the model process the entire input sequence before it produces the output. We can use this architecture to classify text as one of multiple topics. It's a good idea to let the model \"read\" the whole text before it decides what it's about. We will also use the sequence-to-vector architecture for electricity consumption prediction.\n",
    "\n",
    "### 9. Vector-to-sequence architecture\n",
    "One can also build a vector-to-sequence architecture where we pass a single input and replace all other inputs with zeros but make use of all the outputs from each time step. This architecture can be used for text generation: given a single vector representing a specific topic, style, or sentiment, a model can generate a sequence of words or sentences.\n",
    "\n",
    "### 10. Encoder-decoder architecture\n",
    "Finally, in an encoder-decoder architecture, we pass the input sequences, and only then start using the output sequence. This is different from sequence-to-sequence in which outputs are generated while the inputs are still being received. A canonical use case is machine translation. One cannot translate word by word; rather the entire input must be processed before output generation can start.\n",
    "\n",
    "### 11. RNN in PyTorch\n",
    "Let's build a sequence-to-vector RNN in PyTorch. We define a model class with the init method as usual. Inside it, we assign the nn.RNN layer to self.rnn, passing it an input size of 1 since we only have one feature, the electricity consumption, an arbitrarily chosen hidden size of 32 and 2 layers, and we set batch_first to True since our data will have the batch size as its first dimension. We also define a linear layer mapping from the hidden size of 32 to the output of 1. In the forward method, we initialize the first hidden state to zeros using torch.zeros and assign it to h0. Its shape is the number of layers (2) by input size, which we extract from x as x.size-zero, by hidden state size (32). Next, we pass the input x and the first hidden state through the RNN layer. Then, we select only the last output by indexing the middle dimension with -1, pass the result through the linear layer, and return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e398d7-4bf1-4f5d-bfc0-d4837dc4a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize first hidden state with zeros\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Pass x and h0 through recurrent layer\n",
    "        out, _ = self.rnn(x, h0)  \n",
    "        # Pass recurrent layer's last output through linear layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cdb6c6-bd19-48aa-9479-e1282f1ab054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38bd5178-9c75-4c86-96b1-045511fbd54c",
   "metadata": {},
   "source": [
    "### 1. LSTM and GRU cells\n",
    "Let's discuss recurrent architectures more powerful than a plain RNN.\n",
    "\n",
    "### 2. Short-term memory problem\n",
    "Because RNN neurons pass the hidden state from one time step to the next, they can be said to maintain some sort of memory. That's why they are often called RNN memory cells, or just cells for short. However, this memory is very short-term: by the time a long sentence is processed, the hidden state doesn't have much information about its beginning. Imagine trying to translate a sentence between languages; as soon as we have read it, we don't remember how it started. To solve this short-term memory problem, two more powerful types of cells have been proposed: the Long Short-Term Memory or LSTM cell and the Gated Recurrent Unit or GRU cell.\n",
    "\n",
    "### 3. RNN cell\n",
    "Before we look at LSTM and GRU cells, let's visualize the plain RNN cell. At each time step t, it takes two inputs, the current input data x and the previous hidden state h. It multiplies these inputs with the weights, applies activation, and outputs two things: the current outputs y and the next hidden state.\n",
    "\n",
    "### 4. LSTM cell\n",
    "The LSTM cell has three inputs and outputs. Next to the input data x, there are two hidden states: h represents the short-term memory and c the long-term memory. At each time step, h and x are passed through some linear layers called gate controllers which determine what is important enough to keep in the long-term memory. The gate controllers first erase some parts of the long-term memory in the forget gate. Then, they analyze x and h and store their most important parts in the long-term memory in the input gate. This long-term memory, c, is one of the outputs of the cell. At the same time, another gate called the output gate determines what the current output y should be. The short-term memory output h is the same as y.\n",
    "\n",
    "### 5. LSTM in PyTorch\n",
    "Building an LSTM network in PyTorch is very similar to the plain RNN we have already seen. In the init method, we only need to use the nn.LSTM layer instead of nn.RNN. The arguments that the layer takes as inputs are the same. In the forward method, we add the long-term hidden state c and initialize both h and c with zeros. Then, we pass h and c as a tuple to the LSTM layer. Finally, we take the last output, pass it through the linear layer and return just like before.\n",
    "\n",
    "### 6. GRU cell\n",
    "The GRU cell is a simplified version of the LSTM cell. It merges the long-term and short-term memories into a single hidden state. It also doesn't use an output gate: the entire hidden state is returned at each time step.\n",
    "\n",
    "### 7. GRU in PyTorch\n",
    "Building a GRU network in PyTorch is almost identical to the plain RNN. All we need to do is replace the nn.rnn with nn.gru when defining the layer in the init method, and then call the new gru layer in the forward method.\n",
    "\n",
    "### 8. Should I use RNN, LSTM, or GRU?\n",
    "So, which type of recurrent network should we use: the plain RNN, LSTM, or GRU? There is no single answer, but consider the following. Although plain RNNs have revolutionized modeling of sequential data and are important to understand, they are not used much these days because of the short-term memory problem. Our choice will likely be between LSTM and GRU. GRU's advantage is that it's less complex than LSTM, which means less computation. Other than that, the relative performance of GRU and LSTM varies per use case, so it's often a good idea to try both and compare the results. We will learn how to evaluate these models soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6461769-28bb-480c-b95e-f57709eff4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        # Define lstm layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Initialize long-term memory\n",
    "        c0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Pass all inputs to lstm layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd24de35-3ec0-445d-ad32-1e076a0dd60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define RNN layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        out, _ = self.gru(x, h0)  \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22045402-34dc-41e3-a284-3723d88b92e5",
   "metadata": {},
   "source": [
    "### 2. Mean Squared Error Loss\n",
    "Up to now, we have been solving classification tasks using cross-entropy losses. Forecasting of electricity consumption is a regression task, for which we will use a different loss function: Mean Squared Error. Here is how it's calculated. The difference between the predicted value and the target is the error. We then square it, and finally average over the batch of examples. Squaring the errors plays two roles. First, it ensures positive and negative errors don't cancel out, and second, it penalizes large errors more than small ones. Mean Squared Error loss is available in PyTorch as nn.MSELoss.\n",
    "\n",
    "### 3. Expanding tensors\n",
    "Before we take a look at the model training and evaluation, we need to discuss two useful concepts: expanding and squeezing tensors. Let's tackle expanding first. All recurrent layers, RNNs, LSTMs, and GRUs, expect input in the shape: batch size, sequence length, number of features. But as we loop over the DataLoader, we can see that we got the shape batch size of 32 by the sequence length of 96. Since we are dealing with only one feature, the electricity consumption, the last dimension is dropped. We can add it, or expand the tensor, by calling view on the sequence and passing the desired shape.\n",
    "\n",
    "### 4. Squeezing tensors\n",
    "Conversely, as we evaluate the model, we will need to revert the expansion we have applied to the model inputs which can be achieved through squeezing. Let's see why that's the case and how to do it. As we iterate through test data batches, we get labels in shape batch size. Model outputs, however, are of shape batch size by 1, our number of features. We will be passing the labels and the model outputs to the loss function, and each PyTorch loss requires its inputs to be of the same shape. To achieve that, we can apply the squeeze method to the model outputs. This will reshape them to match the labels' shape.\n",
    "\n",
    "### 5. Training loop\n",
    "The training loop is similar to what we have already seen. We instantiate the model and define the loss and the optimizer. Then, we iterate over epochs and training data batches. For each batch, we reshape the input sequence as we have just discussed. The rest of the training loop is the same as before.\n",
    "\n",
    "### 6. Evaluation loop\n",
    "Let's look at the evaluation loop. We start by setting up the Mean Squared Error metric from torchmetrics. Then, we iterate through test data batches without computing the gradients. Next, we reshape the model inputs just like during training, pass them to the model, and squeeze the outputs. Finally, we update the metric. After the loop, we can print the final metric value by calling compute on it, just like we did before.\n",
    "\n",
    "### 7. LSTM vs. GRU\n",
    "Here is our LSTM's test Mean Squared Error again. Let's see how it compares to a GRU network. It seems that for our electricity consumption dataset, with the task defined as predicting the next value based on the previous 24 hours of data, both models perform similarly, with GRU achieving even a slightly lower error. In this case, GRU might be preferred as it achieves the same or better results while requiring less processing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f8a527-fecd-4329-90da-cd96aed1c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "# Set up MSE loss\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(\n",
    "  net.parameters(), lr=0.0001\n",
    ")\n",
    "\n",
    "for epoch in range(3):\n",
    "    for seqs, labels in dataloader_train:\n",
    "        # Reshape model inputs (batch size, sequence length, num features)\n",
    "        seqs = seqs.view(16, 96, 1)\n",
    "        # Get model outputs\n",
    "        outputs = net(seqs)#.squeeze()\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71987010-286f-42d6-abfb-743d64cd2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MSE metric\n",
    "mse = torchmetrics.MeanSquaredError()\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for seqs, labels in dataloader_test:\n",
    "        seqs = seqs.view(32, 96, 1)\n",
    "        # Pass seqs to net and squeeze the result\n",
    "        outputs = net(seqs).squeeze()\n",
    "        mse(outputs, labels)\n",
    "\n",
    "# Compute final metric value\n",
    "test_mse = mse.compute()\n",
    "print(f\"Test MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5921d7d-b179-4e0f-8be7-66e4040c1193",
   "metadata": {},
   "source": [
    "# Multi Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ea145e-0fb9-447b-9a8e-b43fec0f0b0c",
   "metadata": {},
   "source": [
    "### 2. Why multi-input?\n",
    "Multi-input models, or models that accept more than one source of data, have many applications. First, we might want the model to use multiple information sources, such as two images of the same car to predict its model. Second, multi-modal models can work on different input types such as image and text to answer a question about the image. Next, in metric learning, the model learns whether two inputs represent the same object. Think about an automated passport control where the system compares our passport photo with a picture it takes of us. Finally, in self-supervised learning, the model learns data representation by learning that two augmented versions of the same input represent the same object. Multi-input models are everywhere!\n",
    "\n",
    "### 3. Omniglot dataset\n",
    "Throughout the chapter, we will be using the Omniglot dataset, a collection of images of 964 different handwritten characters from 30 different alphabets.\n",
    "\n",
    "### 4. Character classification\n",
    "Let's use the Omniglot dataset to build a two-input model to classify handwritten characters. The first input will be the image of the character, such as this Latin letter \"k\".\n",
    "\n",
    "### 5. Character classification\n",
    "The second input will the the alphabet that it comes from expressed as a one-hot vector.\n",
    "\n",
    "### 6. Character classification\n",
    "Both inputs will be processed separately, then we concatenate their representations.\n",
    "\n",
    "### 7. Character classification\n",
    "Finally a classification layer predicts one of the 964 classes. We need two elements to build such a model: a custom Dataset and an appropriate model architecture.\n",
    "\n",
    "### 8. Two-input Dataset\n",
    "Let's start with the custom Omniglot dataset. We set it up as a class based on torch Dataset. In the init method, we store transform and samples provided when instantiating the dataset as class attributes. Samples are tuples of three: image file path, alphabet as a one-hot vector, and target label as the character class index. In the exercises, samples will be provided. For personal projects, we would need to create them from data file paths. Next, we need to implement the len method that returns the number of samples. Finally, the getitem method returns one sample based on the index it receives as input. For the given index, we retrieve the sample and load the image using Image.open from PIL. The convert method with the argument \"L\" makes sure that the image is read as grayscale. Then, we transform the image and return a triplet: the transformed image, the alphabet vector, and the target label.\n",
    "\n",
    "### 9. Tensor concatenation\n",
    "Before we proceed to building the model, we need to understand tensor concatenation. torch.cat concatenates tensors along a specified dimension. We pass it the tensors and the dimension: for 2D tensors, 0 stands for \"horizontal\" and 1 stands for \"vertical\" concatenation.\n",
    "\n",
    "### 10. Two-input architecture\n",
    "It's time to define our two-input model. We start with defining a sub-network or layer to process our first input, the image. It should look familiar: a convolution, max pool, elu activation, flattened to a linear layer of shape 128 in the end. Next, we define a layer to process our second input, the alphabet vector. Its input size is 30, the number of alphabets, and we map it to an arbitrarily chosen output size of 8. Then, a classifier would accept input of size 128 plus 8 (image and alphabet outputs concatenated) and produce the output of size 964, the number of classes.\n",
    "\n",
    "### 11. Two-input architecture\n",
    "In the forward method, we pass each input through its corresponding layer. Then, we concatenate the outputs with torch.cat. Finally, we pass the result through the classifier layer and return.\n",
    "\n",
    "### 12. Training loop\n",
    "The training loop looks just like all the ones we have seen so far. The only difference is that now the training data consists of three items: the image, the alphabet vector, and the labels, and we pass the images and alphabets to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30f7c2f9-c0f3-4480-9751-830d15706a41",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mOmniglotDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, transform, samples):\n\u001b[0;32m      3\u001b[0m \t\t\u001b[38;5;66;03m# Assign transform and samples to class attributes\u001b[39;00m\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class OmniglotDataset(Dataset):\n",
    "    def __init__(self, transform, samples):\n",
    "\t\t# Assign transform and samples to class attributes\n",
    "        self.transform = transform\n",
    "        self.samples = samples\n",
    "                    \n",
    "    def __len__(self):\n",
    "\t\t# Return number of samples\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      \t# Unpack the sample at index idx\n",
    "        img_path, alphabet, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        # Transform the image \n",
    "        img_transformed = self.transform(img)\n",
    "        return img_transformed, alphabet, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b902ec89-e1f4-4c4f-9a60-00daac45faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Define sub-networks as sequential models\n",
    "        self.image_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*32*32, 128)\n",
    "        )\n",
    "        self.alphabet_layer = nn.Sequential(\n",
    "            nn.Linear(30, 8),\n",
    "            nn.ELU(), \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 + 8, 964), \n",
    "        )\n",
    "        \n",
    "    def forward(self, x_image, x_alphabet):\n",
    "\t\t# Pass the x_image and x_alphabet through appropriate layers\n",
    "        x_image = self.image_layer(x_image)\n",
    "        x_alphabet = self.alphabet_layer(x_alphabet)\n",
    "        # Concatenate x_image and x_alphabet\n",
    "        x = torch.cat((x_image, x_alphabet), dim=1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a81985-671d-4968-b95d-19d0408aec18",
   "metadata": {},
   "source": [
    "### 2. Why multi-output?\n",
    "Just like multi-input models, multi-output architectures are everywhere. Their simplest use-case is for multi-task learning, where we want to predict two things from the same input, such as a car's make and model from its picture. In multi-label classification problem, the input can belong to multiple classes simultaneously. For instance, an image can depict both a beach and people. For each of these labels, a separate output from the model is needed. Finally, in very deep models built of blocks of layers, it is a common practice to add extra outputs predicting the same targets after each block. These additional outputs ensure that the early parts of the model are learning features useful for the task at hand while also serving as a form of regularization to boost the robustness of the network.\n",
    "\n",
    "### 3. Character and alphabet classification\n",
    "Let's use the Omniglot dataset again to build a model to predict both the character and the alphabet it comes from based on the image. First, we will pass the image through some layers to obtain its embedding.\n",
    "\n",
    "### 4. Character and alphabet classification\n",
    "Then we add two independent classifiers on top, one for each output.\n",
    "\n",
    "### 5. Two-output Dataset\n",
    "The good news is that we have already done much of the work needed. We can reuse the OmniglotDataset we built before, with just one small difference in the samples we pass it. When the alphabet was an input to the model, we represented it as a one-hot vector. Now that it is an output, all we need is the integer representing the class label, just like with the other output, the character. This will be a number between 0 and 29 since we have 30 alphabets in the Dataset.\n",
    "\n",
    "### 6. Two-output architecture\n",
    "Let's look at the model's architecture. We start with defining a sub-network for processing the image identical to the one we used before. Then, we define two classifier layers, one for each output, with the output shape corresponding to the number of alphabets (30) and characters (964), respectively. In the forward method, we first pass the image through its dedicated sub-network, and then feed the result separately to each of the two classifiers. Finally, we return the two outputs.\n",
    "\n",
    "### 7. Training loop\n",
    "Let's examine the training loop. The beginning should look familiar, except for the fact that now the model produces two outputs instead of one. Having produced these outputs, we calculate the loss for each of them separately using the appropriate target labels. Next, we need to define the total loss for the model to optimize. Here, we just sum the two partial losses together, indicating that the accuracy of predicting the alphabet and the character is equally important. If that is not the case, we can weigh the partial losses with some weights to reflect their relative importance. We will explore this idea later in the next video. Finally, we run backpropagation and the optimization step as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b575b53-0122-488f-a05a-99049092310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the sample at index 100\n",
    "print(samples[100])\n",
    "\n",
    "# Create dataset_train\n",
    "dataset_train = OmniglotDataset(\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "      \ttransforms.Resize((64, 64)),\n",
    "    ]),\n",
    "    samples=samples,\n",
    ")\n",
    "\n",
    "# Create dataloader_train\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, batch_size=64, shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc670c-930d-464d-aa37-deb10c38e3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.image_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*32*32, 128)\n",
    "        )\n",
    "        # Define the two classifier layers corresponding to the number of alphabets (30) and the number of characters (964)\n",
    "        self.classifier_alpha = nn.Linear(128, 30)\n",
    "\t\tself.classifier_char = nn.Linear(128, 964)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_image = self.image_layer(x)\n",
    "        # Pass x_image through the classifiers and return both results\n",
    "        output_alpha = self.classifier_alpha(x_image)\n",
    "        output_char = self.classifier_char(x_image)\n",
    "        return output_alpha, output_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfabe37-6e35-4fbe-a497-a62dbf01751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.05)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for images, labels_alpha, labels_char in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        outputs_alpha, outputs_char = net(images)\n",
    "        # Compute alphabet classification loss\n",
    "        loss_alpha = criterion(outputs_alpha, labels_alpha)\n",
    "        # Compute character classification loss\n",
    "        loss_char = criterion(outputs_char, labels_char)\n",
    "        # Compute total loss\n",
    "        loss = loss_alpha + loss_char\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb2fd0a-a566-4371-ac4a-8c6a5197fc5a",
   "metadata": {},
   "source": [
    "# Multi-Model evaluation\n",
    "Let's start with the evaluation of a multi-output model. It's very similar to what we have done before. However, with two different outputs, we need to set up two accuracy metrics: one for alphabet classification and one for character classification. We iterate over the test DataLoader and get the model's predictions as usual. Finally, we update the accuracy metrics, and after the loop, we can calculate their final values. The accuracy is higher for alphabets than for characters, which is not surprising: predicting the alphabet is an easier task with just 30 classes to choose from; for characters, there are 964 possible labels. The difference in accuracy scores is not very large, however: 31 versus 24 percent. This is because learning to recognize the alphabets helped the model recognize individual characters: there is a combined positive effect from solving these two tasks at once.\n",
    "\n",
    "### 3. Multi-output training loop revisited\n",
    "Let's now take a look at the training loop for our last model predicting characters and alphabets. Because the model solves two classification tasks at the same time, we have two losses: one for alphabets, and another one for characters. However, since the optimizer can only handle one objective, we had to combine the two losses somehow. We chose to define the final loss as the sum of the two partial losses. By doing so, we are telling the model that recognizing characters and recognizing alphabets are equally important to us. If that is not the case, we can combine the two losses differently.\n",
    "\n",
    "### 4. Varying task importance\n",
    "Let's say that correct classification of characters is twice as important for us as the classification of alphabets. To pass this information to the model, we can multiply the character loss by two to force the model to optimize it more. Another approach is to assign weights to both losses that sum up to one. This is equivalent from the optimization perspective, but arguably easier to read for humans, especially with more than two loss components.\n",
    "\n",
    "### 5. Warning: losses on different scales\n",
    "There is just one caveat: when assigning loss weights, we must be aware of the magnitudes of the loss values. If the losses are not on the same scale, one loss could dominate the other, causing the model to effectively ignore the smaller loss. Consider a scenario where we're building a model to predict house prices, and use MSE loss. If we also want to use the same model to provide a quality assessment of the house, categorized as \"Low\", \"Medium\", or \"High\", we would use cross-entropy loss. Cross-entropy is typically in the single-digit range, while MSE can reach tens of thousands. Combining these two would result in the model ignoring the quality assessment task completely. A solution is to scale each loss by dividing it by the maximum value in the batch. This brings them to the same range, allowing us to weight them if desired and add together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9dedc1-637d-4e34-9af9-577b950fde0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0989cc6-f368-44e0-9ea1-f4509ab62d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcefb97-4954-4956-b3d1-77e016789136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a478db1b-117e-4d99-841a-894f3c9bb846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b409fca-0bca-4e74-9ae4-d144cb8af659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
