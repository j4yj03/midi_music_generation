{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9761ee06-5001-4112-8371-b14880ec45d5",
   "metadata": {},
   "source": [
    "### What we will learn\n",
    "We will explore deep learning using PyTorch for text classification and generation. We'll cover encoding, deep learning models for text, and advanced topics around transformer architecture and protecting our models from attacks. These skills apply to real-world tasks, like sentiment analysis, text summarization, and machine translation.\n",
    "\n",
    "### 3. What you should know\n",
    "Before we begin, you should already be familiar with developing deep learning models with PyTorch, including training and evaluation loops, and have familiarity with convolutional and recurrent neural networks.\n",
    "\n",
    "### 4. Text processing pipeline\n",
    "Welcome to the text processing pipeline! Our text analysis approach in PyTorch involves preprocessing, encoding, and Dataset and DataLoader. This video will focus on preprocessing. We will explore encoding and recap Dataset and Dataloader later in the chapter. Let's begin.\n",
    "\n",
    "### 5. Text processing pipeline\n",
    "In preprocessing, we clean and prepare the text data for encoding.\n",
    "\n",
    "### 6. PyTorch and NLTK\n",
    "Preprocessing raw text data utilizes natural language processing techniques. We'll use PyTorch and NLTK, the Natural Language Toolkit, which provides a range of techniques to transform raw text into processed text.\n",
    "\n",
    "### 7. Preprocessing techniques\n",
    "We will discuss tokenization, stop word removal, stemming, and rare word removal.\n",
    "\n",
    "### 8. Tokenization\n",
    "The first step in text preprocessing is tokenization. This is where we extract tokens from text. A token could be a full word, part of a word, or a punctuation. We'll use the PyTorch get_tokenizer function imported from torchtext-dot-data-dot-utils. The basic_english tokenizer supports the English language. We input the sentence: \"I am reading a book now. I love to read books!\". By applying tokenization, our output becomes a list of tokens.\n",
    "\n",
    "### 9. Stop word removal\n",
    "Next is stopword removal, where NLTK is more suited. Here, we eliminate stopwords or commonly occurring words such as a, the, and, or, and others that don't contribute to the meaning of a text, allowing the model to focus on the words with meaning. We download the stopwords collection of words, also known as corpus, from nltk using nltk-dot-download and import the stopwords package. We create a set of stopwords with no duplicates using stopwords-dot-words. We use English to process English text, but other options are available. With list comprehension, we iterate through the tokens we previously created and filter out any stopwords. Note the use of the lower method; this helps us capture all instances of stopwords regardless of capitalization. Finally, we print the filtered tokens.\n",
    "\n",
    "### 10. Stemming\n",
    "Stemming reduces words or tokens to their base or root form for simplified analysis. For example, \"running,\" \"runs,\" and \"ran\" would all be converted to \"run\" using stemming. We use the NLTK library's PorterStemmer package to perform stemming on a set of words or tokens. We initialize the PorterStemmer. Its input will be a list of tokenized words with stopwords removed. We iterate through this list using stemmer-dot-stem to stem each token. In the output, reading becomes read, and books becomes book.\n",
    "\n",
    "### 11. Rare word removal\n",
    "Lastly, we can remove rare words that occur infrequently and may not provide value for our text analysis. We calculate the word frequencies using the FreqDist function from the nltk-dot-probability module and define the tokens input. We then define a threshold value of two to determine the rare words. We filter out the rare words by keeping only tokens whose frequency exceeds the threshold. Then, we print the result.\n",
    "\n",
    "### 12. Preprocessing techniques\n",
    "The techniques we have covered help refine our text data by reducing the number of features and creating cleaner, more representative datasets. We have only covered a few techniques here. Many more exist but are out of scope for this course. We encourage you to explore these further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c50750-c122-4d52-950c-99d3c40732df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions\n",
    "import nltk\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "text = \"In the city of Dataville, a data analyst named Alex explores hidden insights within vast data. With determination, Alex uncovers patterns, cleanses the data, and unlocks innovation. Join this adventure to unleash the power of data-driven decisions.\"\n",
    "\n",
    "# Initialize the tokenizer and tokenize the text\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "threshold = 1\n",
    "# Remove rare words and print common tokens\n",
    "freq_dist = FreqDist(tokens)\n",
    "common_tokens = [token for token in tokens if freq_dist[token] > threshold]\n",
    "print(common_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119a1d6f-6ecb-4735-adf9-c21cf511fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize and tokenize the text\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "# Remove any stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Perform stemming on the filtered tokens\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ee089-dadd-4e2d-8299-b54852d0b79c",
   "metadata": {},
   "source": [
    "### 2. Text encoding\n",
    "Encoding happens after processing the data. Using PyTorch, we can convert text into machine-readable numbers for analysis and modeling. As seen in the image, each value in the red table is encoded in the blue table.\n",
    "\n",
    "### 3. Encoding techniques\n",
    "We will discuss three encoding methods: One-hot encoding transforms words into unique numerical representations, Bag-of-Words captures word frequency disregarding order, and TF-IDF balances the uniqueness and importance of words in a document. Additionally, embedding converts words into vectors representing semantic meanings. We will review embeddings in the next chapter.\n",
    "\n",
    "### 4. One-hot encoding\n",
    "With one-hot encoding, each word maps onto a distinct one-hot binary vector within the encoding space where one represents the presence of a word and zero the absence. For instance, in a vocabulary consisting of cat, dog, and rabbit, the one-hot vector for 'cat' could be `[1, 0, 0]`, `[0, 1, 0]` for 'dog' and `[0, 0, 1]` for 'rabbit'.\n",
    "\n",
    "### 5. One-hot encoding with PyTorch\n",
    "We have a vocab list that contains input tokens. For sentence input, we tokenize to create a list of tokens. We first determine the vocab list length. Using torch, we utilize the torch-dot-eye function to generate one-hot vectors for the length of our list. We create a dictionary called one_hot_dict where each word is mapped to its corresponding vector from one_hot_vectors. This allows us to easily access the vector representation of any word in our vocabulary.\n",
    "\n",
    "### 6. Bag-of-words\n",
    "Alternatively, we could improve our models by adding more meaning with bag-of-words, which treats a document as an unordered collection of words, emphasizing word frequency over order. For instance, the sentence 'The cat sat on the mat' is converted into a dictionary. In our case, \"the\" is the only word that appears twice.\n",
    "\n",
    "### 7. CountVectorizer\n",
    "In some cases, like this one, sklearn streamlines Bag-of-Words implementation. We import CountVectorizer from sklearn-dot-feature_extraction-dot-text. We instantiate a CountVectorizer object. We define our corpus, a collection of text documents represented here as a list of sentences. This can also be a tokenized list. We fit our vectorizer to the corpus and transform it into a numerical format using fit_transform. This produces our Bag-of-Words representation, which we store in X and print using the toarray function. We can visualize the words by extracting the feature names from the vectorizer with dot-get_feature_names_out. The output is a term frequency matrix, where each row corresponds to a document and each column corresponds to a word. For example, the presence of \"and\" in the first column is indicated by a one in the third row.\n",
    "\n",
    "### 8. TF-IDF\n",
    "The last technique we will cover is TF-IDF or Term Frequency-Inverse Document Frequency. It assesses word importance by considering word frequency across all documents, assigning higher scores to rare words and lower scores to common ones. TF-IDF emphasizes informative words in our text data, unlike bag-of-words, which treats all words equally.\n",
    "\n",
    "### 9. TfidfVectorizer\n",
    "To use TF-IDF we import TfidfVectorizer from sklearn. We instantiate a TfidfVectorizer object using the same corpus as before and fit it like we did for CountVectorizer. This transforms the data into TF-IDF vectors. TF-IDF can also accept a tokenized list. The toarray function yields a matrix of TF-IDF scores. We print the feature names. Every row in the matrix represents a document from the corpus. The feature names list displays the most significant words across all documents, and each word represents a column of the matrix.\n",
    "\n",
    "### 10. TfidfVectorizer\n",
    "For instance, the importance of the word first is highest in the first sentence with a score of zero-point-six-eight.\n",
    "\n",
    "### 11. Encoding techniques\n",
    "Encoding allows models to understand and process text. Ideally, we choose one technique for encoding to avoid redundant computations. As with processing, other encoding techniques exist but are beyond this course's scope. We will cover embeddings in the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb74a3-b5a6-4e34-b266-26e24669fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['Fiction','Non-fiction','Biography', 'Children','Mystery']\n",
    "\n",
    "# Define the size of the vocabulary\n",
    "vocab_size = len(genres)\n",
    "\n",
    "# Create one-hot vectors\n",
    "one_hot_vectors = torch.eye(vocab_size)\n",
    "\n",
    "# Create a dictionary mapping genres to their one-hot vectors\n",
    "one_hot_dict = {genre: one_hot_vectors[i] for i, genre in enumerate(genres)}\n",
    "\n",
    "for genre, vector in one_hot_dict.items():\n",
    "    print(f'{genre}: {vector.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef1f861-f8ad-437d-b840-25a5e058f480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e3c790-b260-43bf-a0fe-0663f9a06096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "titles = ['The Great Gatsby','To Kill a Mockingbird','1984','The Catcher in the Rye','The Hobbit', 'Great Expectations']\n",
    "\n",
    "# Initialize Bag-of-words with the list of book titles\n",
    "vectorizer = CountVectorizer()\n",
    "bow_encoded_titles = vectorizer.fit_transform(titles)\n",
    "\n",
    "# Extract and print the first five features\n",
    "print(vectorizer.get_feature_names_out()[:5])\n",
    "print(bow_encoded_titles.toarray()[0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d021871a-36ad-4ce9-b56b-50f9d0284539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing TF-IDF from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF encoding vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_encoded_descriptions = vectorizer.fit_transform(descriptions)\n",
    "\n",
    "# Extract and print the first five features\n",
    "print(vectorizer.get_feature_names_out()[:5])\n",
    "print(tfidf_encoded_descriptions.toarray()[0, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b161f0-4c40-4d37-9c70-82ff7803ddc8",
   "metadata": {},
   "source": [
    "### 2. Recap: preprocessing\n",
    "The first pipeline component is preprocessing. Recall the techniques we reviewed are tokenization, stopword removal, stemming, and rare word removal. These actions help to reduce the complexity of our models.\n",
    "\n",
    "### 3. Text processing pipeline\n",
    "The second component is encoding. Here, we convert our preprocessed text into numerical vectors using methods like One-Hot Encoding, Bag-of-Words, or TF-IDF. This enables our models to understand and process textual data. Another technique is embeddings, which will be discussed in the next chapter.\n",
    "\n",
    "### 4. Text processing pipeline\n",
    "We complete our pipeline by using PyTorch's Dataset and DataLoader. In our text processing pipeline, we will use Dataset as a container for our processed and encoded text data. DataLoader then allows us to iterate over this dataset in batches, shuffle the data, and apply multiprocessing for efficient loading.\n",
    "\n",
    "### 5. Recap: implementing Dataset and DataLoader\n",
    "Let's review applying Dataset and DataLoader to text data in PyTorch. We create a custom class, TextDataset, serving as our data container. The init method initializes the dataset with the input text data. The len method returns the total number of samples in the dataset, and the getitem method allows us to access a specific sample at a given index. This class, extending PyTorch's Dataset, allows us to organize and access our text data efficiently.\n",
    "\n",
    "### 6. Recap: integrating Dataset and DataLoader\n",
    "After encoding our text data, we instantiate our TextDataset with the encoded text. We then create a DataLoader, making the dataset iterable.\n",
    "\n",
    "### 7. Using helper functions\n",
    "For convenience, we'll use helper functions for preprocessing and encoding. preprocess_sentences combines the techniques we've covered; we can also customize it to only include specific techniques depending on the problem. We've chosen CountVectorizer in encode_sentences to convert the cleaned sentences into arrays. We've included an extract_sentences function that uses regular expressions (regex) to convert English sentences. While regex is beyond the scope of this course, we've included it here for potential use in the pre-exercise code.\n",
    "\n",
    "### 8. Constructing the text processing pipeline\n",
    "Now, let's construct our text processing pipeline. We define a function text_processing_pipeline that takes raw text as input. Within this function, we preprocess the text using the preprocess_sentences function. This returns a list of tokens. Next, we convert these tokens into numerical vectors using the encode_sentences function. After encoding, we instantiate our PyTorch TextDataset with the numerical vectors, then initialize a DataLoader with this dataset. The DataLoader will allow us to iterate over the dataset in manageable batches of size two and in a shuffled manner, ensuring a diverse mix of examples in each batch.\n",
    "\n",
    "### 9. Applying the text processing pipeline\n",
    "With our text processing pipeline function ready, we can apply it to any text data. Let's say we have two sentences: \"This is the first text data\" and \"And here is another one\". We call the extract sentences function to convert the text to sentences. We feed each of these sentences into our text_processing_pipeline function. This preprocesses, encodes, and loads them into individual DataLoaders, stored in the dataloaders list using list comprehension. We also store an instance of the vectorizer created during encoding to access the feature names for each vector. Finally, the print statement uses the next iter combination and allows us to access the batches of data from the dataloaders. The output is the first ten components of the first batch in the dataloader. It contains the encoded representation of the sentences that represent the frequency of the first five words in the vocabulary for each sentence.\n",
    "\n",
    "### 10. Text processing pipeline: it's a wrap!\n",
    "Excellent work! Our text processing pipeline efficiently converts raw text data into a machine-learning-ready format. After processing the text through this pipeline, we can use the resulting structured data to train, validate, and test models. We'll apply this pipeline to large datasets in upcoming chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2578597e-b428-4330-94f1-68508c92abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Initialize the tokenizer and stemmer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "# Complete the function to preprocess sentences\n",
    "def preprocess_sentences(sentences):\n",
    "    processed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokens = tokenizer(sentence)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        processed_sentences.append(' '.join(tokens))\n",
    "    return processed_sentences\n",
    "\n",
    "processed_shakespeare = preprocess_sentences(shakespeare)\n",
    "print(processed_shakespeare[:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc0f31-0f7c-4b45-bbfc-bdb533f1a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sentences(data):\n",
    "    return re.findall(r'[A-Z][^.!?]*[.!?]', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6386308-47ae-4ff9-879c-6de160627444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a9624-7fa5-46fb-8db9-f14eae5b35a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your Dataset class\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Complete the encoding function\n",
    "def encode_sentences(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    return X.toarray(), vectorizer\n",
    "    \n",
    "# Complete the text processing pipeline\n",
    "def text_processing_pipeline(sentences):\n",
    "    processed_sentences = preprocess_sentences(sentences)\n",
    "    encoded_sentences, vectorizer = encode_sentences(processed_sentences)\n",
    "    dataset = ShakespeareDataset(encoded_sentences)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    return dataloader, vectorizer\n",
    "\n",
    "dataloader, vectorizer = text_processing_pipeline(processed_shakespeare)\n",
    "\n",
    "# Print the vectorizer's feature names and the first 10 components of the first item\n",
    "print(vectorizer.get_feature_names_out()[:10]) \n",
    "print(next(iter(dataloader))[0, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec474eb-e1a0-46b9-9d33-2ea8471c4526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fbc1cef-5038-454d-af2c-152d9bb8e103",
   "metadata": {},
   "source": [
    "### Text classification defined\n",
    "Text classification assigns labels to text, giving meaning to words and sentences. It helps in organizing and giving structure to unstructured data and is crucial in various applications, such as analyzing customer sentiment in reviews, detecting spam emails, or tagging news articles with topics. We'll cover three classification types: binary, multi-class, and multi-label.\n",
    "\n",
    "### 3. Binary classification\n",
    "Binary classification sorts text into two categories, such as spam and not spam, as seen in email spam detection.\n",
    "\n",
    "### 4. Multi-class classification\n",
    "Multi-class classification categorizes text into more than two categories. For example, a news article could be classified into one of various categories, like politics, sports, or technology, depending on its content.\n",
    "\n",
    "### 5. Multi-label classification\n",
    "In multi-label classification, text can belong to multiple categories simultaneously, unlike multi-class where it belongs to just one category. For example, a book can fit into multiple genres like action, adventure, and fantasy all at the same time.\n",
    "\n",
    "### 6. What are word embeddings\n",
    "Classifying text sometimes requires an understanding of the meaning of words. Previously, we covered encoding techniques including one-hot, bag-of-words, and TF-IDF in the processing pipeline. While these techniques are fundamental to preprocessing and are a good first step to extracting features, they often result in too many features and can't identify similar words. In contrast, word embeddings represent words as numerical vectors, preserving semantic meanings and connections between words like king and queen or man and woman. While the diagram shows three-dimensional vectors, real-world word embeddings often have much higher dimensionality.\n",
    "\n",
    "### 7. Word to index mapping\n",
    "To enable embedding, we assign a unique index to a word with word-to-index mapping. For instance, we can translate \"King\" to one and \"Queen\" to two, giving us a numerical representation that is more compact and computationally efficient compared to the previous encoding techniques. Word-to-index mapping typically follows tokenization in the text processing pipeline, but it can follow any of the preproccessing techniques we've covered.\n",
    "\n",
    "### 8. Word embeddings in PyTorch\n",
    "PyTorch's torch-dot-nn-dot-Embedding is a flexible tool for creating word embeddings. It takes word indexes and transforms them into word vectors or embeddings. For example, given our sentence \"The cat sat on the mat\", it produces a unique vector for each word based on its index. Initially, these lists, or embeddings, contain random numbers because the model hasn't learned the meanings of the words yet. Through training, these embeddings start to change and learn, helping the model understand word meanings and their relationships.\n",
    "\n",
    "### 9. Using torch.nn.Embedding\n",
    "Let's implement word embeddings using PyTorch's nn-dot-Embedding. First, we construct our words list. We employ a dictionary, word-to-idx, to map words to indexes by enumerating the words through a dictionary comprehension. Utilizing the torch-dot-LongTensor function, we represent these mapped values as a tensor. We define an embedding layer with num_embeddings argument set to the length of the words list. embedding_dim specifies the size of each embedding vector; we've set it to ten. Remember, embedding_dim is a hyperparameter that can be increased to tune our results further, but as we have only six words, we have chosen the smallest embedding. We create a tensor of indexes to pass through the embedding layer. The output is an embedding for each input value. In our case, it gives us a two-dimensional tensor with six rows and ten columns. Each row contains the embedding for the corresponding word.\n",
    "\n",
    "### 10. Using embeddings in the pipeline\n",
    "Here is what this step would look like in the full pipeline with the dataset and dataloader we previously created. Here, we perform our embedding on the data generated by the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce0e8d-9aaa-49ac-a9cb-44c2e8f0491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map a unique index to each word\n",
    "words = [\"This\", \"book\", \"was\", \"fantastic\", \"I\", \"really\", \"love\", \"science\", \"fiction\", \"but\", \"the\", \"protagonist\", \"was\", \"rude\", \"sometimes\"]\n",
    "word_to_idx = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "# Convert word_to_idx to a tensor\n",
    "inputs = torch.LongTensor([word_to_idx[w] for w in words])\n",
    "\n",
    "# Initialize embedding layer with ten dimensions\n",
    "embedding = nn.Embedding(num_embeddings=len(words), embedding_dim=10)\n",
    "\n",
    "# Pass the tensor to the embedding layer\n",
    "output = embedding(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a387c6-ef15-414c-b1b2-950a78b30066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cba9fd3-2e05-44bb-9208-a15f6f836128",
   "metadata": {},
   "source": [
    "### 2. CNNs for text classification\n",
    "We have seen CNNs used for classifying images, but they can also apply to text, for example, for classifying tweets as positive, negative, or neutral.\n",
    "\n",
    "### 3. The convolution operation\n",
    "The key operation in CNNs is convolution, where a filter or kernel slides over the input, performing element-wise calculations. This helps the model learn detailed word and sentence structure and meaning in text data.\n",
    "\n",
    "### 4. Filter and stride in CNNs\n",
    "In the convolution operation, we use a filter, a small matrix that slides over the input data, a matrix of tensors. We use a parameter called stride, which determines how many positions the filter moves each time it slides. In the image, we move a two by two filter with a stride of two.\n",
    "\n",
    "1 Animation from Vincent Dumoulin, Francesco Visin\n",
    "### 5. CNN architecture for text\n",
    "A typical CNN architecture for text classification consists of three layers. The convolutional layer applies filters to the input data to detect patterns. The pooling layer reduces the size of the data while preserving important information. Finally, the fully connected layer uses the previous layer outputs for final predictions.\n",
    "\n",
    "### 6. Implementing a text classification model using CNN\n",
    "Let's build a sentiment analysis model, starting with the SentimentAnalysisCNN class. Much of the code will look familiar, and we'll prepare the dataset in later steps. The init method accepts vocabulary size and embedding dimension to configure the network architecture. The super method initializes the base class of nn-dot-Module to leverage the PyTorch framework properly. We initialize an embedding layer using nn-dot-Embedding, which creates dense vectors with the specified vocabulary size and embed_dim. In our case, self-dot-conv directly initializes a single convolutional layer, while in other models, convolutional layers are grouped and applied sequentially. Convolutions apply nn-dot-Conv1d, with uniform input-output channels, kernel size, stride, and padding, which ensures uniform text sequence lengths. Conv1d is preferred over Conv2d as our text data is one-dimensional. Lastly, the nn-dot-Linear layer transforms the combined outputs of all convolutional layers into the desired target output size. We omit a pooling layer in this model because our data in the exercises will be small.\n",
    "\n",
    "### 7. Implementing a text classification model using CNN\n",
    "In the forward method, we pass the input text through an embedding layer, which converts each word to its embedding. The tensor's dimensions are permuted to match the convolutional layer's expected input format defined with batch size, embedding size, and sequence length, in our case zero, two, and one, respectively. We use the convolutional layer with a ReLU activation function to extract important features from the embeddings. Applying the activation function in forward allows dynamic computation that saves memory compared to defining in init. conved-dot-mean calculates the average across the sequence length to reduce the feature dimension and capture the essential information, simplifying the information in each sentence to a single average value for easier analysis by the model.\n",
    "\n",
    "### 8. Preparing data for the sentiment analysis model\n",
    "To prepare our data, we create a vocabulary and use word to index mapping. While One-Hot or TF-IDF encoding methods are alternatives, they are less efficient as they do not capture contextual word relationships and result in high dimensional input vectors mostly filled with zeros. Hence, we opt for embeddings. We set vocab-size to the length of word to index and embed-dim to ten. We have two book review samples for demonstration. We then initialize our SentimentAnalysisCNN model with vocab_size for vocabulary size and embed_dim for word embedding dimension. For training, we use Cross-Entropy loss with Stochastic Gradient Descent as our optimizer, setting a learning rate of zero-point-one.\n",
    "\n",
    "### 9. Training the model\n",
    "During ten training epochs, we iterate over each sentence-label pair in the data, clearing previous gradients at the model level for clean computation. Words in sentences are mapped to indexes using word_to_idx and converted to a long tensor. We use unsqueeze zero to add an extra dimension to the start of the tensor, creating a batch containing a single sequence to fit the model's input expectations. The model then predicts sentiments, and we turn the label into a long tensor. We compute the loss between predictions and actual labels, calculate gradients via backpropagation, and adjust the model parameters using the optimizer.\n",
    "\n",
    "### 10. Running the Sentiment Analysis Model\n",
    "With our model and data ready, we can start making predictions. We iterate over book_samples, transforming the words to tensors, and feed it to the model. The output provides sentiment scores for our classification labels. Using torch-dot-max, we identify the component with the highest score. One corresponds to positive sentiment, while zero corresponds to negative sentiment. We then print the review alongside its predicted sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862216a8-ad28-460b-9da1-47b359cd2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(TextClassificationCNN, self).__init__()\n",
    "        # Initialize the embedding layer \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(embed_dim, 2)\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text).permute(0, 2, 1)\n",
    "        # Pass the embedded text through the convolutional layer and apply a ReLU\n",
    "        conved = F.relu(self.conv(embedded))\n",
    "        conved = conved.mean(dim=2) \n",
    "        return self.fc(conved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4014f3ac-6654-4aa5-8d2c-a3d54eb6bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for sentence, label in data:     \n",
    "        # Clear the gradients\n",
    "        model.zero_grad()\n",
    "        sentence = torch.LongTensor([word_to_ix.get(w, 0) for w in sentence]).unsqueeze(0) \n",
    "        label = torch.LongTensor([int(label)])\n",
    "        outputs = model(sentence)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa0df0e-227d-4c25-8a77-46036bf09bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_reviews = [\n",
    "    \"I love this book\".split(),\n",
    "    \"I do not like this book\".split()\n",
    "]\n",
    "for review in book_reviews:\n",
    "    # Convert the review words into tensor form\n",
    "    input_tensor = torch.tensor([word_to_ix[w] for w in review], dtype=torch.long).unsqueeze(0) \n",
    "    # Get the model's output\n",
    "    outputs = model(input_tensor)\n",
    "    # Find the index of the most likely sentiment category\n",
    "    _, predicted_label = torch.max(outputs.data, 1)\n",
    "    # Convert the predicted label into a sentiment string\n",
    "    sentiment = \"Positive\" if predicted_label.item() else \"Negative\"\n",
    "    print(f\"Book Review: {' '.join(review)}\")\n",
    "    print(f\"Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f677078-5b8b-4b90-aec5-3e2b2a906520",
   "metadata": {},
   "source": [
    "# RNNs for text\n",
    "Recurrent Neural Networks, or RNNs, are great at handling sequences of varying lengths. They maintain an internal short-term memory, enabling them to learn patterns across time. Unlike CNNs that spot patterns in chunks of text, RNNs remember past words to understand the whole sentence's meaning. Today, we will explore how to employ RNNs for text classification.\n",
    "\n",
    "### 3. RNNs for text classification\n",
    "RNNs are suitable for text classification because they process sequential data like humans read, one word at a time, allowing them to capture the context and order of words. Consider the tweet, \"I just love getting stuck in traffic\"; RNNs can accurately classify the tweet as sarcastic.\n",
    "\n",
    "### 4. Recap: Implementing Dataset and DataLoader\n",
    "Let's remind ourselves how to apply Dataset and DataLoader for text data in PyTorch. We create a custom class TextDataset, serving as our data container. The init method initializes the dataset with the input text data. The len method returns the total number of samples in the dataset, and the getitem method allows us to access a specific sample at a given index. This class, extending PyTorch's Dataset, allows us to organize and access our text data efficiently.\n",
    "\n",
    "### 5. RNN implementation\n",
    "Now let's take a look at an example of sentiment analysis for movie review from a tweet. We want to train an RNN model to classify movie reviews as either positive or negative. We can use our entire text processing pipeline here to feed to the model. This includes encoding or embedding. We preprocess the tweet and convert it to a tensor, which is not shown here for brevity. Then, we pass the preprocessed tensor through the model to make a sentiment prediction. In this case, the model predicts that the sentiment is \"Positive.\"\n",
    "\n",
    "### 6. RNN variation: LSTM\n",
    "But what if the tweet is not so straightforward to understand the sentiment. Take the tweet, \"Loved the cinematography, hated the dialogue. The acting was exceptional, but the plot fell flat\". These complex sentences contain subtle nuances and conflicting sentiments. While RNNs may struggle to capture the negative sentiment, Long Short Term Memory models or LSTMs excel at capturing such complexities. They can effectively understand the underlying emotions, making them a powerful tool for sentiment analysis.\n",
    "\n",
    "### 7. LSTM\n",
    "LSTMs have input, forget, and output gates that enable them to store and forget information as needed. This architecture is ideal for complex classification tasks. The code defines an LSTM model using nn-dot-LSTM, with an initialization function that sets the input size, hidden size, and batch-first parameter. The forward function processes the input through the LSTM layer using self-dot-lstm, and the rest is similar to RNN.\n",
    "\n",
    "### 8. RNN variation: GRU\n",
    "But, what if we wanted to detect spam emails without needing the full context. Given an email subject like \"Congratulations! You've won a free trip to Hawaii!\", a Gated Recurrent Unit or GRU, can quickly recognize spammy patterns without needing the full context. This makes them suitable for tasks like spam detection, sentiment analysis, text summarization, and more.\n",
    "\n",
    "### 9. GRU\n",
    "GRUs are a streamlined version of LSTMs that trade some complexity for faster training. The code defines a GRU model using nn-dot-GRU, with an initialization function that specifies the input size, hidden size, and batch-first parameter. The forward function remains the same, with the change of self-dot-lstm becoming self-dot-gru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60acd5-297d-4c1d-be46-00de662f6629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the RNN class\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "rnn_model = RNNModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model for ten epochs and zero the gradients\n",
    "for epoch in range(10): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = rnn_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ab64b-ef33-4dbb-97ba-5b0541d8e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LSTM and the output layer with parameters\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)       \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Initialize model with required parameters\n",
    "lstm_model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model by passing the correct parameters and zeroing the gradient\n",
    "for epoch in range(10): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = lstm_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913df848-d751-48d6-ac12-6dc43ff406ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)       \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size) \n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = out[:, -1, :] \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "gru_model = GRUModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gru_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model and backpropagate the loss after initialization\n",
    "for epoch in range(15): \n",
    "    optimizer.zero_grad()\n",
    "    outputs = gru_model(X_train_seq)\n",
    "    loss = criterion(outputs, y_train_seq)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff84fcb-bbfc-48e5-983f-b7f3dcd121ca",
   "metadata": {},
   "source": [
    "### 2. Why evaluation metrics matter\n",
    "Picture this: Our model, designed to assess the sentiment of book reviews, suggests that a best-seller has mostly negative reviews. Should we accept its judgment? We can use evaluation metrics to answer this.\n",
    "\n",
    "### 3. Evaluation RNN Models\n",
    "Before evaluating, we must generate predictions from the model. First, we pass the test dataset through the model to obtain the output predictions for each class. Next, we store the predictions in the predicted variable using the torch-dot-max function that returns the indexes of the maximum values along the specified dimension, indicated by the argument one. We'll use the predicted variable for evaluation metrics.\n",
    "\n",
    "### 4. Accuracy\n",
    "The most straightforward metric is accuracy, the ratio of correct predictions to the total predictions. Using torchmetrics, the tensors actual represent our actual labels, and predicted the model predictions. We want to determine if an instance belongs to class zero or class one, a binary classification. The accuracy class is initialized with a binary task and num_classes set to two for our two categories. The task can also be multiclass if there are more than two categories to classify. Passing labels to the accuracy instance gives the model's accuracy score. A score of zero-point-66 indicates the model predicted just over 66 percent of the samples correctly. A good score can vary based on the complexity of the problem. Scores range from zero to one, with higher scores representing greater accuracy. For example, zero-point-75 may be reasonable for sentiment analysis but poor elsewhere. As we learn more about metrics, we'll see that accuracy alone doesn't capture everything.\n",
    "\n",
    "### 5. Beyond accuracy\n",
    "Imagine a dataset of 10,000 book reviews where 9,800 readers adore the book and 200 found faults. Let's assume our model predicts all instances as positive, making it 98% accurate! But look closer. Such a model can't classify a single negative sentiment. Enter precision, which questions the model's confidence in labeling a review as negative. Recall checks how well the model spots actual negative reviews. The F1 Score harmonizes these two, ensuring neither is neglected. If we were to trust accuracy alone, we'd miss significant feedback. Let's explore each in more detail.\n",
    "\n",
    "### 6. Precision and Recall\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positives. Recall is the ratio of correctly predicted positive observations to all observations in the actual positive class. To calculate these, we import the Precision and Recall classes from torchmetrics, use the same parameters as before, and print the results.\n",
    "\n",
    "### 7. Precision and Recall\n",
    "A precision of zero-point-six-six suggests that out of all positive predictions, just over 66 percent were accurate. Meanwhile, a recall of zero-point-five signifies the model captured 50 percent of all genuine positives. Like accuracy, the scores range from zero to one. The complexity of the problem needs to be considered when defining a score as good or bad.\n",
    "\n",
    "### 8. F1 score\n",
    "The F1 Score harmonizes precision and recall and is especially useful when dealing with imbalanced classes. To calculate it, we import the F1 Score class from torchmetrics and instantiate it with the same parameters. An F1 Score of one indicates perfect precision and recall, while a score of zero indicates the worst possible performance. Here F1 Score of zero-point-57 suggests a reasonably balanced trade-off between precision and recall, but this trade-off will depend on the task.\n",
    "\n",
    "### 9. Considerations\n",
    "In some instances, such as with multi-class classification, we may find that all scores are identical. Generally, this indicates a model is performing well across all classes. But remember to always consider the problem when interpreting results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75081541-702c-4d17-9407-f727ab9964db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the metrics\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "precision = Precision(task=\"multiclass\", num_classes=num_classes)\n",
    "recall = Recall(task=\"multiclass\", num_classes=num_classes)\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "# Generate the predictions\n",
    "outputs = rnn_model(X_test_seq)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy_score = accuracy(predicted, y_test_seq)\n",
    "precision_score = precision(predicted, y_test_seq)\n",
    "recall_score = recall(predicted, y_test_seq)\n",
    "f1_score = f1(predicted, y_test_seq)\n",
    "print(\"RNN Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_score, precision_score, recall_score, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbee2e3-16aa-4f39-bcb4-b987723a6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the metrics\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=3)\n",
    "precision = Precision(task=\"multiclass\", num_classes=3)\n",
    "recall = Recall(task=\"multiclass\", num_classes=3)\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=3)\n",
    "\n",
    "# Calculate metrics for the LSTM model\n",
    "accuracy_1 = accuracy(y_pred_lstm, y_test)\n",
    "precision_1 = precision(y_pred_lstm, y_test)\n",
    "recall_1 = recall(y_pred_lstm, y_test)\n",
    "f1_1 = f1(y_pred_lstm, y_test)\n",
    "print(\"LSTM Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_1, precision_1, recall_1, f1_1))\n",
    "\n",
    "# Calculate metrics for the GRU model\n",
    "accuracy_2 = accuracy(y_pred_gru, y_test)\n",
    "precision_2 = precision(y_pred_gru, y_test)\n",
    "recall_2 = recall(y_pred_gru, y_test)\n",
    "f1_2 = f1(y_pred_gru, y_test)\n",
    "print(\"GRU Model - Accuracy: {}, Precision: {}, Recall: {}, F1 Score: {}\".format(accuracy_2, precision_2, recall_2, f1_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be7086-9df9-4f59-a76a-c5d0fb1b1ba8",
   "metadata": {},
   "source": [
    "### 2. Text generation and NLP\n",
    "Text generation is utilized in Natural Language Processing, serving various applications like chatbots, translations, and technical writing. RNNs, LSTMs, and GRUs ability to remember past information positions them as a key technology for processing sequential text data. For example, given the input \"The cat is on the m\", an RNN would complete the statement with \"at\", for \"mat\".\n",
    "\n",
    "### 3. Building an RNN for text generation\n",
    "To construct an RNN text generation model, we import the essential libraries: torch and nn. Initiating a \"Hello how are you?\" data variable, we extract unique characters and establish bidirectional mappings (character to index and vice versa) to convert textual data to numerical form, and subsequently back to text post-prediction. Character-to-integer conversions are favored over words for reduced dimensionality and computational ease, especially in smaller datasets, ensuring the model efficiently processes numerical input and generates readable text outputs. The init method takes input size, hidden size, and output size. Hidden size is stored as an instance variable. An RNN layer with specified dimensions is defined, followed by a fully connected layer. Our goal will be to train a model to generate \"Hello how are you?\". In training to predict subsequent characters, inputting 'h' should suggest 'e' as a likely next character if \"he\" is a frequent bigram.\n",
    "\n",
    "### 4. Forward propagation and model creation\n",
    "In the forward method, we initialize a tensor of zeros for the initial hidden state, providing a neutral starting point for the RNN to learn from the data. We then pass the input data and the initial hidden state into the RNN layer to generate an output sequence and a new hidden state. We extract the last time step's output from the RNN and process it through a fully connected layer. This step allows us to convert the RNN's output into a format suitable as the next element in a generated sequence. We return this output as the result of the forward method. We instantiate our RNNmodel with an input size of one, a hidden size of 16, and an output size of one, optimized for single-token input and output sequences while allowing a 16-unit hidden layer for feature extraction. We treat text generation as a regression problem to predict the next token in a sequence as the next token can have infinite tensor output classes rather than a set number of output classes needed for classification. Our loss function is CrossEntropyLoss. We use the Adam optimizer, specifying the learning rate as 0-point-01.\n",
    "\n",
    "### 5. Preparing input and target data\n",
    "The inputs and targets lists are created by mapping each character in the data string to its corresponding index, excluding the last character for inputs and the first character for targets. The index lists are then converted into long tensors. Additionally, we reshape inputs to have an additional dimension and match the expected input shape for the model. The inputs tensor is one-hot encoded, turning each index into a binary vector, where all elements are zero except for the one at the position of the index. However, the targets tensor remains as character indices to align with CrossEntropyLoss, which requires class indices as targets.\n",
    "\n",
    "### 6. Training the RNN model\n",
    "We initiate our training loop for 100 epochs, switch our model to training mode, and feed the inputs to the model and get the outputs. We calculate the loss by comparing the model's outputs to the actual targets. As PyTorch accumulates gradients, we clear the existing gradients in the optimizer. We perform backpropagation to compute loss gradients for the model parameters, and update them. Finally, we print the epoch number and the current loss every ten epochs. The output is shown on the next slide.\n",
    "\n",
    "### 7. Testing the model\n",
    "Let's test our trained RNN model. We switch the model to evaluation mode. We will prepare the character 'h' for prediction. 'h' is converted to its index using character to index mapping. The nn-dot-functional-dot-one_hot function is used to one hot encode the index. The tensor is reshaped to a compatible format for encoding, with num_classes set to the length of unique characters and converted to a float tensor. We feed test_input into the model to get the predicted_output. Using torch-dot-argmax on this output, we find the index of the maximum value along axis one, representing the most probable next character. We then print the model's prediction for this input. The decreasing loss over 100 epochs suggests our model learned and improved. When we input h into our trained model, it predicted e, which is fairly close. This indicates that our model has learned to generate text well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20383d3c-e933-4c27-8d03-a28fc4d30a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include an RNN layer and linear layer in RNNmodel class\n",
    "class RNNmodel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNmodel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "      h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "      out, _ = self.rnn(x, h0)  \n",
    "      out = self.fc(out[:, -1, :])  \n",
    "      return out\n",
    "\n",
    "# Instantiate the RNN model\n",
    "model = RNNmodel(len(chars), 16, len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de761282-5707-4adb-a274-cdc3e7e845d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Instantiate the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/100, Loss: {loss.item()}')\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "test_input = char_to_ix['r']\n",
    "test_input = nn.functional.one_hot(torch.tensor(test_input).view(-1, 1), num_classes=len(chars)).float()\n",
    "predicted_output = model(test_input)\n",
    "predicted_char_ix = torch.argmax(predicted_output, 1).item()\n",
    "print(f\"Test Input: 'r', Predicted Output: '{ix_to_char[predicted_char_ix]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b2f50-a0ee-4ce2-83c4-5ec38456ca3a",
   "metadata": {},
   "source": [
    "### 1. Generative adversarial networks for text generation\n",
    "Generative Adversarial Networks, or GANs, are often used for image generation\n",
    "\n",
    "### 2. GANs and their role in text generation\n",
    "but are becoming more common for text generation for creating synthetic data that preserves statistical similarities. Unlike RNNs, GANs replicate complex data patterns, ensuring feature correlation and authentically emulating real-world patterns.\n",
    "\n",
    "### 3. Structure of a GAN\n",
    "A GAN consists of two primary components: the Generator, which creates synthetic text data from noise, and the Discriminator, which distinguishes between real and generated text data. Here, noise refers to random changes to real data, such as adding special characters to a word. These components collaborate, with the Generator improving its fakes and the Discriminator enhancing its ability to detect them until the generated text becomes indistinguishable from real text.\n",
    "\n",
    "### 4. Building a GAN model in PyTorch: Generator\n",
    "We begin building a GAN model by defining the Generator. Our data is product reviews that have been embedded and converted to tensors, not shown here for brevity. The goal is for our model to create believable reviews. We define our Generator network with nn-dot-Module. It has a linear layer inside the Sequential function that transforms the input to have the same dimension as our data sequences. It is followed by a sigmoid activation function suitable for binary data that squashes the output values to the range zero to one. The forward method then applies this network to an input tensor.\n",
    "\n",
    "### 5. Building the discriminator network\n",
    "We define a Discriminator network similarly. This network has a linear layer that transforms the input to a single value, followed by a sigmoid activation function. The output represents the probability that the input data is real. The forward method applies this network to an input tensor.\n",
    "\n",
    "### 6. Initializing networks and loss function\n",
    "We initialize our Generator and Discriminator network instances and define the loss function as Binary Cross Entropy for binary classification tasks like distinguishing between real and fake data. Next, we create two Adam optimizers for the Generator and the Discriminator. Each optimizer has a learning rate 0-point-001, a value often used as a starting point and may be adjusted based on model performance.\n",
    "\n",
    "### 7. Training the discriminator\n",
    "We establish a training loop for 50 epochs, generating batches of real data and random noise for the Generator to create fake data. We obtain predictions from the Discriminator for real and fake data, using the detach function to prevent gradient tracking. The Discriminator's loss is calculated using torch-dot-ones_like and torch-dot-zeros_like to match the expected real and fake labels. We reset the gradients in the Discriminator's optimizer with zero_grad, perform backpropagation to calculate gradients, and update the Discriminator's parameters.\n",
    "\n",
    "### 8. Training the generator\n",
    "Next we train the Generator. We calculate the Generator's loss based on how well it fooled the Discriminator. The loss is determined by the difference between the Discriminator's predictions on fake data and an array of ones. We then reset the gradients in the Generator's optimizer, perform backpropagation to calculate gradients, and update the Generator's parameters. We print Generator and Discriminator losses every ten epochs to monitor training progress.\n",
    "\n",
    "### 9. Printing real and generated data\n",
    "After the training is complete, we print some real data. Then, we sample random values to form inputs for the Generator, generating data points mirroring the real data distribution.\n",
    "\n",
    "### 10. GANs: generated synthetic data\n",
    "The displayed output reveals Generator and Discriminator losses for every 10th epoch, demonstrating a consistent decline. However, after 50 epochs, the losses remain high, indicating the need for further training.\n",
    "\n",
    "### 11. Generated data\n",
    "Here's what our model generated. Since the input data was in tensor form, the output is also in tensor format. Upon reviewing the matrix, the real and generated data are similar. In practice, we would assess this further by plotting a correlation matrix and checking if the correlation between columns is maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596f86a-4b34-48b5-a9c9-f5df6434a925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e853bd4a-318e-4a8b-8885-414a865a06f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d616e-a134-4a13-933f-89d098ffacae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f958810-0f04-4f6a-93c6-7e7b3e5a47e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b7ebcc-0dc8-4c09-88a4-d6077523f6be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d82d22d-22e4-40d6-8518-1a93bf9fffcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72182589-4b11-40a1-83d1-62d7b734ea43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
